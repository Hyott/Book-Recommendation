import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
from sklearn.preprocessing import normalize
from numpy.linalg import norm 
import os
print("os.getcwd: ", os.getcwd())
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..")))
from psycopg2 import sql
from sqlalchemy.orm import Session
from app.database.models import SentenceTable
import json


def get_centroid_after_round5(sorted_cluster_books, book_embeddings, centroid_weight, cluster_of_winner_5):
    """
    í´ëŸ¬ìŠ¤í„° ê¸°ë°˜ ê°€ì¤‘ í‰ê·  ì¤‘ì‹¬ì  ê³„ì‚°
    """
    # ğŸ“Œ í´ëŸ¬ìŠ¤í„° ë° ì±… ì¸ë±ìŠ¤ ì •ë¦¬
    book_values_of_cluster = list(sorted_cluster_books.values())
    book_keys_of_cluster = list(sorted_cluster_books.keys())

    print("book_values_of_cluster:", book_values_of_cluster)
    print("book_keys_of_cluster:", book_keys_of_cluster)

    first_cluster_num = cluster_of_winner_5
    first_cluster_indices = sorted_cluster_books[first_cluster_num]

    # ğŸ›‘ í´ëŸ¬ìŠ¤í„° ì¶”ì¶œ ë¡œì§ ìˆ˜ì • (pop ëŒ€ì‹  ì°¨ì§‘í•© ì‚¬ìš©)
    remaining_clusters = [key for key in book_keys_of_cluster if key != first_cluster_num]

    if len(first_cluster_indices) == 3:
        second_cluster_num = remaining_clusters[0]
        third_cluster_num = remaining_clusters[1]

        weight_first = centroid_weight
        weight_second = 0.5 * (1 - weight_first)
        weight_third = 0.5 * (1 - weight_first)

    else:
        second_cluster_num = remaining_clusters[0]
        third_cluster_num = remaining_clusters[1]

        weight_first = centroid_weight
        weight_second = 0.67 * (1 - weight_first)
        weight_third = 0.33 * (1 - weight_first)

    second_cluster_indices = sorted_cluster_books[second_cluster_num]
    third_cluster_indices = sorted_cluster_books[third_cluster_num]

    print(f"first_cluster_num: {first_cluster_num}")
    print(f"second_cluster_num: {second_cluster_num}")
    print(f"third_cluster_num: {third_cluster_num}")
    print(f"weight_first: {weight_first}, weight_second: {weight_second}, weight_third: {weight_third}")
    print(f"first_cluster_indices : {first_cluster_indices}")
    print(f"second_cluster_indices : {second_cluster_indices}")
    print(f"third_cluster_indices : {third_cluster_indices}")


    # âœ… NumPy ìŠ¬ë¼ì´ì‹± ìµœì í™”
    centroid_first = np.mean(book_embeddings[first_cluster_indices], axis=0)
    centroid_second = np.mean(book_embeddings[second_cluster_indices], axis=0)
    centroid_third = np.mean(book_embeddings[third_cluster_indices], axis=0)

    # âœ… ê°€ì¤‘ í‰ê·  ê³„ì‚° ìˆ˜ì • (weight_second ì‚¬ìš©)
    weighted_centroid = (
        centroid_first * weight_first +
        centroid_second * weight_second +
        centroid_third * weight_third
    ) / (weight_first + weight_second + weight_third)

    # ğŸ›¡ï¸ NumPy ë°°ì—´ ë³€í™˜ ë° ì°¨ì› ì •ë¦¬
    if isinstance(weighted_centroid, list):
        weighted_centroid = np.array(weighted_centroid)

    if len(weighted_centroid.shape) == 1:
        weighted_centroid = weighted_centroid.reshape(1, -1)

    print("1ìœ„ í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì :", centroid_first)
    print("2ìœ„ í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì :", centroid_second)
    print("3ìœ„ í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì :", centroid_third)
    print("ê°€ì¤‘ í‰ê·  ì¤‘ì‹¬ì :", weighted_centroid)
    print(f"shape of weighted_centroid: {weighted_centroid.shape}")

    return weighted_centroid

def neighborhood_based_clustering(weighted_centroid, book_embeddings, num_indices, num_cluster):
    # âœ… 1. ì…ë ¥ ë²¡í„° ì°¨ì› í™•ì¸
    assert weighted_centroid.shape[1] == book_embeddings.shape[1], "ì°¨ì›ì´ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."

    # âœ… 2. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ë° ìƒìœ„ 300ê°œ ì„ íƒ
    cosine_similarities = cosine_similarity(book_embeddings, weighted_centroid).flatten()
    
    # âœ… ì„±ëŠ¥ í–¥ìƒ: np.argpartition() ì‚¬ìš©
    num_indices = np.argpartition(cosine_similarities, -num_indices)[-num_indices:]
    selected_vectors = book_embeddings[num_indices]

    # âœ… 3. ì„ íƒëœ ë²¡í„° ì •ê·œí™” (ì½”ì‚¬ì¸ ê±°ë¦¬ ê¸°ë°˜ KMeans)
    normalized_vectors = normalize(selected_vectors)

    # âœ… 4. KMeans í´ëŸ¬ìŠ¤í„°ë§ (ìµœì‹  ì˜µì…˜ ì ìš©)
    num_clusters = num_cluster
    kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init='auto')
    clusters = kmeans.fit_predict(normalized_vectors)

    # ê° í´ëŸ¬ìŠ¤í„°ì˜ ì±… ì¸ë±ìŠ¤ ì €ì¥
    neigh_based_clustering_to_books = {i: [] for i in range(num_clusters)}
    for idx, cluster_id in enumerate(clusters):
        neigh_based_clustering_to_books[cluster_id].append(idx)

    # âœ… 5. í´ëŸ¬ìŠ¤í„° ê²°ê³¼ ì¶œë ¥
    unique, counts = np.unique(clusters, return_counts=True)
    print("K-Means í´ëŸ¬ìŠ¤í„° ê²°ê³¼:")
    for label, count in zip(unique, counts):
        print(f"í´ëŸ¬ìŠ¤í„° {label}: {count}ê°œ")

    return neigh_based_clustering_to_books, num_indices, kmeans, normalized_vectors

import traceback
import numpy as np

def select_books_for_new_cluster(neigh_based_clustering_to_books, top_300_indices, 
                                weighted_centroid, normalized_vectors, 
                                kmeans, question_number, visited_clusters, 
                                selected_books_of_round678):
    
    # weighted_centroid = np.array(weighted_centroid, copy=True)
    print(f"visited_clusters: {visited_clusters}")
    print(f"All Clusters: {list(neigh_based_clustering_to_books.keys())}")
    print(f"âœ… Remaining Clusters: {[c for c in neigh_based_clustering_to_books.keys() if c not in visited_clusters]}")
    print(f"âœ… weighted_centroid.shape: {weighted_centroid.shape}")
    
    try:
        print(f"\nğŸŸ¡ [Start] Selecting books for Question {question_number}")
        print(f"â¡ï¸ weighted_centroid.shape: {weighted_centroid.shape}")
        print(f"â¡ï¸ normalized_vectors.shape: {normalized_vectors.shape}")
        print(f"â¡ï¸ visited_clusters before selection: {visited_clusters}")

        # âœ… 1ì°¨ ì„ íƒ: weighted_centroid ê¸°ë°˜ (question_number == 5)
        if question_number == 5 or question_number == 8 :
            print("ğŸ”¹ Step 1: Finding cluster from weighted_centroid")
            centroid_cluster = int(kmeans.predict(weighted_centroid)[0])
            print(f"âœ… centroid_cluster from weighted_centroid: {centroid_cluster}")
            
            visited_clusters.add(centroid_cluster)
            print(f"ğŸ”¹ Step 2: visited_clusters updated: {visited_clusters}")

            if centroid_cluster not in neigh_based_clustering_to_books:
                raise ValueError(f"centroid_cluster {centroid_cluster} not found in cluster_to_books")

            book_a_index = np.random.choice(neigh_based_clustering_to_books[centroid_cluster])
            print(f"âœ… book_a_index from centroid cluster: {book_a_index}")
            book_a = int(top_300_indices[book_a_index])

        else:
            # âœ… 2ì°¨ ë° 3ì°¨ ì„ íƒ: ê°€ì¥ í° ë¯¸ë°©ë¬¸ í´ëŸ¬ìŠ¤í„° ê¸°ë°˜
            print("ğŸ”¹ Step 3: Selecting largest unvisited cluster for book_a")
            unvisited_clusters = sorted(
                [(cluster, len(books)) for cluster, books in neigh_based_clustering_to_books.items()
                    if cluster not in visited_clusters],
                key=lambda x: x[1],
                reverse=True
            )
            print(f"ğŸ“Š Unvisited Clusters (by size): {unvisited_clusters}")

            if unvisited_clusters:
                largest_cluster = unvisited_clusters[0][0]
                visited_clusters.add(largest_cluster)
                print(f"âœ… Largest unvisited cluster for book_a: {largest_cluster}")
                
                book_a_index = np.random.choice(neigh_based_clustering_to_books[largest_cluster])
                print(f"âœ… book_a_index from largest cluster: {book_a_index}")
                book_a = int(top_300_indices[book_a_index])
            else:
                print("âš ï¸ No unvisited clusters found for book_a")
                book_a = None

        # âœ… book_b: ë‹¤ë¥¸ ë¯¸ë°©ë¬¸ í´ëŸ¬ìŠ¤í„° ì¤‘ ê°€ì¥ í° í´ëŸ¬ìŠ¤í„°
        print("ğŸ”¹ Step 4: Selecting largest remaining unvisited cluster for book_b")
        remaining_clusters = sorted(
            [(cluster, len(books)) for cluster, books in neigh_based_clustering_to_books.items()
                if cluster not in visited_clusters],
            key=lambda x: x[1],
            reverse=True
        )
        print(f"ğŸ“Š Remaining Unvisited Clusters: {remaining_clusters}")

        if remaining_clusters:
            largest_cluster = remaining_clusters[0][0]
            visited_clusters.add(largest_cluster)
            print(f"âœ… Largest unvisited cluster for book_b: {largest_cluster}")
            
            book_b_index = np.random.choice(neigh_based_clustering_to_books[largest_cluster])
            print(f"âœ… book_b_index from largest remaining cluster: {book_b_index}")
            book_b = int(top_300_indices[book_b_index])
        else:
            print("âš ï¸ No remaining unvisited clusters found for book_b")
            book_b = None

        # âœ… ì„ íƒ ê²°ê³¼ ì „ì—­ ë³€ìˆ˜ì— ê¸°ë¡
        print("ğŸ”¹ Step 5: Recording selected books")
        selected_books_of_round678.append(book_a)
        selected_books_of_round678.append(book_b)

        # âœ… ìµœì¢… ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ¯ [ì„ íƒ ì™„ë£Œ - Question {question_number}]")
        print(f"ğŸ“– Book A: {book_a}")
        print(f"ğŸ“– Book B: {book_b}")
        print(f"âœ… Final Visited Clusters: {visited_clusters}\n")

        return book_a, book_b

    except Exception as e:
        print(f"\nâŒ Exception occurred during book selection (Question {question_number})")
        traceback.print_exc()  # ìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ì¶œë ¥
        raise  # ì˜ˆì™¸ ì¬ë°œìƒ


def get_tournament_winner_cluster_until_round5(book_embeddings, cluster_to_books, alpha, beta_values, 
                                        presented_books, exploration_prob, noise_factor, book_choice, 
                                        question_number, books_chosen, suggested_books):
    if question_number == 0:
        book_a = int(np.random.choice([
            idx for idx in cluster_to_books[0] if idx not in presented_books]))
        
        book_b = int(np.random.choice([
            idx for idx in cluster_to_books[1] if idx not in presented_books]))

    elif question_number == 1:
        book_a = int(np.random.choice([
            idx for idx in cluster_to_books[2] if idx not in presented_books]))
        
        book_b = int(np.random.choice([
            idx for idx in cluster_to_books[3] if idx not in presented_books]))
        
    elif question_number == 2:
        book_a = int(np.random.choice([
            idx for idx in cluster_to_books[4] if idx not in presented_books]))
        
        book_b = int(np.random.choice([
            idx for idx in cluster_to_books[5] if idx not in presented_books]))

    elif question_number == 3:
        winner_of_1 = books_chosen[0]
        cluster_of_winner_1 = None

        for cluster_id, book_list in cluster_to_books.items():
            if winner_of_1 in book_list:
                cluster_of_winner_1 = cluster_id
                break
        print("winner_of_1: ", winner_of_1)
        print("cluster_of_winner_1", cluster_of_winner_1)


        winner_of_2 = books_chosen[1]
        cluster_of_winner_2 = None

        for cluster_id, book_list in cluster_to_books.items():
            if winner_of_2 in book_list:
                cluster_of_winner_2 = cluster_id
                break
        print("winner_of_2: ", winner_of_2)
        print("cluster_of_winner_2", cluster_of_winner_2)

        book_a = int(np.random.choice([
            idx for idx in cluster_to_books[cluster_of_winner_1] if idx not in presented_books]))
        book_b = int(np.random.choice([
            idx for idx in cluster_to_books[cluster_of_winner_2] if idx not in presented_books]))
        
    elif question_number == 4:
        winner_of_3 = books_chosen[2]
        cluster_of_winner_3 = None

        for cluster_id, book_list in cluster_to_books.items():
            if winner_of_3 in book_list:
                cluster_of_winner_3 = cluster_id
                break
        print("winner_of_3: ", winner_of_3)
        print("cluster_of_winner_3", cluster_of_winner_3)

        winner_of_4 = books_chosen[3]
        cluster_of_winner_4 = None

        for cluster_id, book_list in cluster_to_books.items():
            if winner_of_4 in book_list:
                cluster_of_winner_4 = cluster_id
                break
        print("winner_of_4: ", winner_of_4)
        print("cluster_of_winner_4", cluster_of_winner_4)

        book_a = int(np.random.choice([
            idx for idx in cluster_to_books[cluster_of_winner_3] if idx not in presented_books]))
        book_b = int(np.random.choice([
            idx for idx in cluster_to_books[cluster_of_winner_4] if idx not in presented_books]))
           

    else:
        raise ValueError("Question number out of range 5")
    
    winner_of_q = {f"winner_of_{question_number}"}

    print("suggested_books : ", suggested_books)
    print("presented_books : ", presented_books)
    print("books_chosen : ", books_chosen)
    presented_books.add(book_a)
    presented_books.add(book_b)
    suggested_books.append(book_a)
    suggested_books.append(book_b) 

    return book_a, book_b


def get_choice_bool(cursor, user_id, question_number):
    cursor.execute(
        sql.SQL("SELECT * FROM public.user_responses WHERE user_id = %s and question_number = %s"),
        (user_id, question_number)
    )
    exists = cursor.fetchall()
    if exists:
        book_a_select = exists[0][4]
        book_b_select = exists[1][4]
    else:
        book_a_select = None
        book_b_select = None
    return book_a_select, book_b_select


def get_sentence_from_db(db: Session):
    """
    ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì±… ë°ì´í„°(id, isbn, sentence)ë§Œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
    """
    sentences = db.query(SentenceTable.id, SentenceTable.isbn, SentenceTable.sentence).all()
    
    if not sentences:
        return []

    return [
        {
            "id": sentence.id,
            "isbn": sentence.isbn,
            "sentence": sentence.sentence,
        }
        for sentence in sentences
    ]

def load_embeddings(file_path):

    with open(file_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    embeddings = [inner_dict["embedding"] for inner_dict in data.values()]
    ids = np.arange(1, len(embeddings) + 1)
    print("len(ids)!!!!!!!!!!!!!!!!!!!!!!!", len(ids))
    return ids, embeddings


def update_data(choice, book_a, book_b, alpha, beta_values):
    """
    ì‚¬ìš©ì ì„ íƒ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë² íƒ€ ë¶„í¬ ì—…ë°ì´íŠ¸.
    """
    print("book_a:", book_a)
    print("book_b:", book_b)
    if choice == "a":
        alpha[book_a] += 1
        beta_values[book_b] += 1
        # alpha[-1] += 1
        # alpha[0] += 1
        print(book_a)
        return book_a
    elif choice == "b":
        alpha[book_b] += 1
        beta_values[book_a] += 1
        return book_b
    else:
        beta_values[book_a] += 1
        beta_values[book_b] += 1
        return None
        
def get_message_by_id(ids, book_id, book_data):
    """
    idsì™€ book_dataë¥¼ ì´ìš©í•´ íŠ¹ì • book_idì˜ ë©”ì‹œì§€ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    idx = np.where(ids == book_id)[0][0]  # book_idì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
    return book_data[idx]["sentence"]

def weighted_sampling(similarities, num_samples=10, temperature=0.5):
    """
    ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í™•ë¥ ì  ìƒ˜í”Œë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    - similarities: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë°°ì—´
    - num_samples: ì¶”ì²œí•  ì±…ì˜ ê°œìˆ˜
    - temperature: ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜ ì¡°ì •ì„ ìœ„í•œ íŒŒë¼ë¯¸í„° (ë‚®ì„ìˆ˜ë¡ ìƒìœ„ ì„ íƒ ì§‘ì¤‘)
    """
    # ìœ ì‚¬ë„ë¥¼ ê°€ì¤‘ì¹˜ë¡œ ë³€í™˜
    probabilities = np.exp(similarities / temperature)
    probabilities /= probabilities.sum()  # í™•ë¥ ë¡œ ì •ê·œí™”

    # ê°€ì¤‘ì¹˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëœë¤ ìƒ˜í”Œë§
    sampled_indices = np.random.choice(len(similarities), size=num_samples, replace=False, p=probabilities)
    return sampled_indices
