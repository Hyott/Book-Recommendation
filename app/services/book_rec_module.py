import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
from sklearn.preprocessing import normalize
from numpy.linalg import norm 
import os
print("os.getcwd: ", os.getcwd())
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..")))
from psycopg2 import sql
from sqlalchemy.orm import Session
from app.database.models import SentenceTable
import json


def get_choice_bool(cursor, user_id, question_number):
    cursor.execute(
        sql.SQL("SELECT * FROM public.user_responses WHERE user_id = %s and question_number = %s"),
        (user_id, question_number)
    )
    exists = cursor.fetchall()
    if exists:
        book_a_select = exists[0][4]
        book_b_select = exists[1][4]
    else:
        book_a_select = None
        book_b_select = None
    return book_a_select, book_b_select


def get_sentence_from_db(db: Session):
    """
    ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì±… ë°ì´í„°(id, isbn, sentence)ë§Œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
    """
    sentences = db.query(SentenceTable.id, SentenceTable.isbn, SentenceTable.sentence).all()
    
    if not sentences:
        return []

    return [
        {
            "id": sentence.id,
            "isbn": sentence.isbn,
            "sentence": sentence.sentence,
        }
        for sentence in sentences
    ]

# === ì„ë² ë”© ë¡œë“œ í•¨ìˆ˜ ===
def load_embeddings(file_path):

    with open(file_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    embeddings = [inner_dict["embedding"] for inner_dict in data.values()]
    ids = np.arange(1, len(embeddings) + 1)
    print("len(ids)!!!!!!!!!!!!!!!!!!!!!!!", len(ids))
    return ids, embeddings

def thompson_sampling(alpha, beta_values):
    """
    Thompson Samplingì„ ìˆ˜í–‰í•˜ì—¬ ê° ì±…ì˜ í™•ë¥  ê°’ì„ ìƒ˜í”Œë§.
    """
    return np.random.beta(alpha, beta_values)


def select_books(book_embeddings, cluster_to_books, alpha, beta_values, presented_books, exploration_prob, noise_factor, book_choice):
    """
    í´ëŸ¬ìŠ¤í„° ê¸°ë°˜ìœ¼ë¡œ ì±… ìŒ ì„ íƒ.
    - íƒìƒ‰(Exploration)ê³¼ í™œìš©(Exploitation)ì˜ ê· í˜•ì„ ë§ì¶° ì±…ì„ ì„ íƒ.
    - ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ì—¬ ì„ íƒì— ë‹¤ì–‘ì„±ì„ ë¶€ì—¬.
    """

    samples = thompson_sampling(alpha, beta_values)

        # ë…¸ì´ì¦ˆ ì¶”ê°€
    noise = np.random.normal(0, noise_factor, size=len(samples))
    noisy_samples = samples + noise

    # if book_choice == None:
    if 0.18 < exploration_prob:
    # Thompson Samplingì„ í†µí•´ ê° ì±…ì˜ ìƒ˜í”Œë§ ê°’ ê³„ì‚°
        # í´ëŸ¬ìŠ¤í„°ë³„ ëŒ€í‘œ ì±… ì„ íƒ (í™•ë¥  ê°’ì´ ë†’ì€ ì±…)
        representative_books = []
        # ì •ìˆ˜ â†’ ì§‘í•©(set) ë³€í™˜
        if isinstance(presented_books, int):
            presented_books = {presented_books}
        # print("type(presented_books): ", type(presented_books))
        # print("presented_books: ", presented_books)

        for cluster_id, books_in_cluster in cluster_to_books.items():
            # print(f"type(books_in_cluster): {type(books_in_cluster)}")
            # print(f"books_in_cluster: {books_in_cluster}")
            cluster_samples = [(idx, noisy_samples[idx]) for idx in books_in_cluster if idx not in presented_books]
            if cluster_samples:
                best_book = max(cluster_samples, key=lambda x: x[1])
                representative_books.append(best_book)

        # ëŒ€í‘œ ì±…ë“¤ ì¤‘ ê°€ì¥ ë†’ì€ ìƒ˜í”Œë§ ê°’ì„ ê°€ì§„ ì±… 1ê°œ ì„ íƒ
        if not representative_books:
            raise ValueError("No more books to present. All books have been used.")

        best_book_a = max(representative_books, key=lambda x: x[1])[0]

    elif exploration_prob <= 0.18:
    # if book_choice:
        # print(f"book_embeddings shape: {book_embeddings.shape}")
        # print(f"book_embeddings[book_choice] shape: {book_embeddings[book_choice].shape}")
        
        similarities = []
        for idx in range(len(book_embeddings)):
            if idx not in presented_books and idx != book_choice:
                # ë²¡í„°ë¥¼ 1Dë¡œ ë³€í™˜
                vector_a = book_embeddings[book_choice].reshape(-1)
                vector_b = book_embeddings[idx].reshape(-1)

                # Norm ê°’ ê³„ì‚°
                norm_choice = norm(vector_a)
                norm_idx = norm(vector_b)

                # Norm ê°’ì´ 0ì´ë©´ ê±´ë„ˆëœ€
                if norm_choice == 0 or norm_idx == 0:
                    continue

                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                similarity = np.dot(vector_a, vector_b) / (norm_choice * norm_idx)
                similarities.append((idx, similarity))

        # ê°€ì¥ ìœ ì‚¬í•œ ì±… ì„ íƒ
        if not similarities:
            raise ValueError("No valid books to calculate similarity. Check presented_books and book_choice.")

        best_book_a = max(similarities, key=lambda x: x[1])[0]


    # íƒìƒ‰ ì—¬ë¶€ ê²°ì •
    if 0.18 < exploration_prob :  ## 1~5ë¼ìš´ë“œê¹Œì§€ë¡œ í•œì •
        # íƒìƒ‰: ì§€ê¸ˆê¹Œì§€ ì„ íƒëœ ì ì´ ì—†ëŠ” í´ëŸ¬ìŠ¤í„° ì¤‘ í•˜ë‚˜ ì„ íƒ
        
        unvisited_clusters = [
            cluster_id for cluster_id, books_in_cluster in cluster_to_books.items()
            if any(idx not in presented_books for idx in books_in_cluster)
        ]

        if unvisited_clusters:
            random_cluster = np.random.choice(unvisited_clusters)
            random_book_b = np.random.choice([
                idx for idx in cluster_to_books[random_cluster] if idx not in presented_books and idx != best_book_a
            ])
        else:
            # ëª¨ë“  í´ëŸ¬ìŠ¤í„°ê°€ ë°©ë¬¸ëœ ê²½ìš°, í™œìš©ìœ¼ë¡œ ì „í™˜
            exploration_prob = 0  # íƒìƒ‰ ë¹„ì¤‘ì„ ì œê±°í•˜ê³  í™œìš©ìœ¼ë¡œ ì´ë™
            
    elif exploration_prob <= 0.18:
        # í™œìš©: ì„ í˜¸ë„ê°€ ë†’ì€ ì±… ì„ íƒ (ë…¸ì´ì¦ˆ ì ìš©)
        random_book_b = max(
            [(idx, noisy_samples[idx]) for idx in range(len(alpha)) if idx not in presented_books and idx != best_book_a],
            key=lambda x: x[1]
        )[0]
    
    # ì¤‘ë³µ ë°©ì§€
    presented_books.add(best_book_a)
    presented_books.add(random_book_b)

    return best_book_a, random_book_b

# def get_centroid_after_round5(sorted_cluster_books, book_embeddings, centroid_weight, cluster_of_winner_5):
#     """
#     1ìœ„ í´ëŸ¬ìŠ¤í„°ì˜ ì±… ìˆ˜ê°€ 3ê°œì¸ ê²½ìš°ëŠ” 2ìœ„ ê°œìˆ˜ëŠ” 1ê°œì´ê³  3ìœ„ëŠ” 1ê°œì´ë‹¤. -> sorted ëŠ”  ë™ë¥ ì¼ ê²½ìš°, ê°’ì´ ì‘ì€ ìˆœìœ¼ë¡œ ì •ë ¬ì´ë¯€ë¡œ, ìµœì¢…ì—°ì‚°ì—ì„œ 2ìœ„ì™€ 3ìœ„ì˜ êµ¬ë¶„ì´ ì—†ì„ ì˜ˆì •ì´ë‹¤.
#     1ìœ„ í´ëŸ¬ìŠ¤í„°ì˜ ì±… ìˆ˜ê°€ 2ê°œì¸ ê²½ìš°, 2ìœ„ì˜ ê°œìˆ˜ëŠ” 2ê°œì´ê³  3ìœ„ëŠ” 1ê°œì´ë‹¤. -> 2ìœ„ì— weightë¥¼ 3ìœ„ë³´ë‹¤ ë§ì´ ì¤„ ì˜ˆì •ì´ë‹¤.
#     """

#     book_values_of_cluster = list(sorted_cluster_books.values())
#     book_keys_of_cluster = list(sorted_cluster_books.keys())
#     print("book_values_of_cluster : ", book_values_of_cluster)
#     print(f"book_keys_of_cluster : {book_keys_of_cluster}")

#     first_cluster_num = cluster_of_winner_5
#     first_cluster_indices = book_values_of_cluster[cluster_of_winner_5]

#     # ğŸ›‘ í´ëŸ¬ìŠ¤í„° ì¶”ì¶œ ë¡œì§ ìˆ˜ì • (pop ëŒ€ì‹  ì°¨ì§‘í•© ì‚¬ìš©)
#     remaining_clusters = [key for key in book_keys_of_cluster if key != first_cluster_num]

#     if len(book_values_of_cluster[cluster_of_winner_5]) == 3:
#         book_keys_of_cluster.pop(cluster_of_winner_5)
#         print(f"book_keys_of_cluster after pop: {book_keys_of_cluster}")
#         second_cluster_num = book_keys_of_cluster[0]
#         third_cluster_num = book_keys_of_cluster[1]
#         second_cluster_indices = book_values_of_cluster[second_cluster_num]
#         third_cluster_indices = book_values_of_cluster[third_cluster_num]

#         # 2ìœ„ì™€ 3ìœ„ ëª¨ë‘ ì„ íƒëœ ê²ƒì€ 1ë²ˆë¿ì´ë¯€ë¡œ ê°€ì¤‘ì¹˜ëŠ” ê°™ë‹¤
#         weight_first = centroid_weight
#         weight_second = 1 - weight_first * 0.5
#         weight_third = 1 - weight_first * 0.5

#     else:
#         second_cluster_num = book_keys_of_cluster[1]
#         third_cluster_num = book_keys_of_cluster[2]
#         second_cluster_indices = book_values_of_cluster[second_cluster_num]
#         third_cluster_indices = book_values_of_cluster[third_cluster_num]

#         weight_first = centroid_weight
#         weight_second = 1 - weight_first * 0.66
#         weight_third = 1 - weight_first * 0.34
    
#     print(f"first_cluster_num : {first_cluster_num}")
#     print(f"second_cluster_num: {second_cluster_num}")
#     print(f"third_cluster_num: {third_cluster_num}")
    

#     print("type of book_embeddings: ", type(book_embeddings))
#     print("shape of book_embeddings: ", book_embeddings.shape)

#         # ê° í´ëŸ¬ìŠ¤í„°ë³„ ë²¡í„° í‰ê·  (centroid)
#     centroid_first = np.mean([book_embeddings[idx] for idx in first_cluster_indices], axis=0)
#     centroid_second = np.mean([book_embeddings[idx] for idx in second_cluster_indices], axis=0)
#     centroid_third = np.mean([book_embeddings[idx] for idx in third_cluster_indices], axis=0)

#     # ê°€ì¤‘ í‰ê·  (Weighted Centroid)
#     weighted_centroid = (centroid_first * weight_first + centroid_second * weight_first + centroid_third * weight_third) / (weight_first + weight_second + weight_third)


#     if isinstance(weighted_centroid, list):
#         weighted_centroid = np.array(weighted_centroid)

# # 2ï¸âƒ£ 1ì°¨ì› ë°°ì—´ì„ 2ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜
#     if len(weighted_centroid.shape) == 1:
#         weighted_centroid = weighted_centroid.reshape(1, -1)

#     print("1ìœ„ í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì :", centroid_first)
#     print("2ìœ„ í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì :", centroid_second)
#     print("ê°€ì¤‘ í‰ê·  ì¤‘ì‹¬ì :", weighted_centroid)
#     print(f"shape of weighted_centroid:  {weighted_centroid.shape} ")

#     return weighted_centroid



# import numpy as np

def get_centroid_after_round5(sorted_cluster_books, book_embeddings, centroid_weight, cluster_of_winner_5):
    """
    í´ëŸ¬ìŠ¤í„° ê¸°ë°˜ ê°€ì¤‘ í‰ê·  ì¤‘ì‹¬ì  ê³„ì‚°
    """
    # ğŸ“Œ í´ëŸ¬ìŠ¤í„° ë° ì±… ì¸ë±ìŠ¤ ì •ë¦¬
    book_values_of_cluster = list(sorted_cluster_books.values())
    book_keys_of_cluster = list(sorted_cluster_books.keys())

    print("book_values_of_cluster:", book_values_of_cluster)
    print("book_keys_of_cluster:", book_keys_of_cluster)

    first_cluster_num = cluster_of_winner_5
    first_cluster_indices = sorted_cluster_books[first_cluster_num]

    # ğŸ›‘ í´ëŸ¬ìŠ¤í„° ì¶”ì¶œ ë¡œì§ ìˆ˜ì • (pop ëŒ€ì‹  ì°¨ì§‘í•© ì‚¬ìš©)
    remaining_clusters = [key for key in book_keys_of_cluster if key != first_cluster_num]

    if len(first_cluster_indices) == 3:
        second_cluster_num = remaining_clusters[0]
        third_cluster_num = remaining_clusters[1]

        weight_first = centroid_weight
        weight_second = 0.5 * (1 - weight_first)
        weight_third = 0.5 * (1 - weight_first)

    else:
        second_cluster_num = remaining_clusters[0]
        third_cluster_num = remaining_clusters[1]

        weight_first = centroid_weight
        weight_second = 0.67 * (1 - weight_first)
        weight_third = 0.33 * (1 - weight_first)

    second_cluster_indices = sorted_cluster_books[second_cluster_num]
    third_cluster_indices = sorted_cluster_books[third_cluster_num]

    print(f"first_cluster_num: {first_cluster_num}")
    print(f"second_cluster_num: {second_cluster_num}")
    print(f"third_cluster_num: {third_cluster_num}")
    print(f"weight_first: {weight_first}, weight_second: {weight_second}, weight_third: {weight_third}")
    print(f"first_cluster_indices : {first_cluster_indices}")
    print(f"second_cluster_indices : {second_cluster_indices}")
    print(f"third_cluster_indices : {third_cluster_indices}")


    # âœ… NumPy ìŠ¬ë¼ì´ì‹± ìµœì í™”
    centroid_first = np.mean(book_embeddings[first_cluster_indices], axis=0)
    centroid_second = np.mean(book_embeddings[second_cluster_indices], axis=0)
    centroid_third = np.mean(book_embeddings[third_cluster_indices], axis=0)

    # âœ… ê°€ì¤‘ í‰ê·  ê³„ì‚° ìˆ˜ì • (weight_second ì‚¬ìš©)
    weighted_centroid = (
        centroid_first * weight_first +
        centroid_second * weight_second +
        centroid_third * weight_third
    ) / (weight_first + weight_second + weight_third)

    # ğŸ›¡ï¸ NumPy ë°°ì—´ ë³€í™˜ ë° ì°¨ì› ì •ë¦¬
    if isinstance(weighted_centroid, list):
        weighted_centroid = np.array(weighted_centroid)

    if len(weighted_centroid.shape) == 1:
        weighted_centroid = weighted_centroid.reshape(1, -1)

    print("1ìœ„ í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì :", centroid_first)
    print("2ìœ„ í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì :", centroid_second)
    print("3ìœ„ í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì :", centroid_third)
    print("ê°€ì¤‘ í‰ê·  ì¤‘ì‹¬ì :", weighted_centroid)
    print(f"shape of weighted_centroid: {weighted_centroid.shape}")

    return weighted_centroid



# import numpy as np

# def get_centroid_after_round5(sorted_cluster_books, book_embeddings, centroid_weight, cluster_of_winner_5):
#     """
#     í´ëŸ¬ìŠ¤í„° ê¸°ë°˜ ê°€ì¤‘ í‰ê·  ì¤‘ì‹¬ì  ê³„ì‚°
#     """
#     # ğŸ“Œ í´ëŸ¬ìŠ¤í„° ë° ì±… ì¸ë±ìŠ¤ ì •ë¦¬
#     book_values_of_cluster = list(sorted_cluster_books.values())
#     book_keys_of_cluster = list(sorted_cluster_books.keys())

#     print("ğŸ“Š book_values_of_cluster:", book_values_of_cluster)
#     print("ğŸ“Š book_keys_of_cluster:", book_keys_of_cluster)

#     # âœ… ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬
#     if not book_values_of_cluster or len(book_values_of_cluster) < 3:
#         print("âŒ í´ëŸ¬ìŠ¤í„° ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ ë°˜í™˜.")
#         return np.zeros((1, book_embeddings.shape[1]))

#     # âœ… cluster_of_winner_5 ìœ íš¨ì„± í™•ì¸
#     if cluster_of_winner_5 >= len(book_values_of_cluster):
#         print(f"âŒ cluster_of_winner_5({cluster_of_winner_5})ê°€ í´ëŸ¬ìŠ¤í„° ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤.")
#         return np.zeros((1, book_embeddings.shape[1]))

#     first_cluster_num = cluster_of_winner_5
#     first_cluster_indices = book_values_of_cluster[first_cluster_num]

#     # âœ… remaining_clusters ê³„ì‚°
#     remaining_clusters = [key for key in book_keys_of_cluster if key != first_cluster_num]

#     # âœ… remaining_clusters ê¸¸ì´ ì ê²€
#     if len(remaining_clusters) < 2:
#         print("âŒ remaining_clustersì˜ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ê¸°ë³¸ê°’ ë°˜í™˜.")
#         return np.zeros((1, book_embeddings.shape[1]))

#     if len(first_cluster_indices) == 3:
#         second_cluster_num = remaining_clusters[0]
#         third_cluster_num = remaining_clusters[1]

#         weight_first = centroid_weight
#         weight_second = 1 - weight_first * 0.5
#         weight_third = 1 - weight_first * 0.5
#     else:
#         second_cluster_num = remaining_clusters[0]
#         third_cluster_num = remaining_clusters[1]

#         weight_first = centroid_weight
#         weight_second = 1 - weight_first * 0.66
#         weight_third = 1 - weight_first * 0.34

#     # âœ… second_cluster_indices, third_cluster_indices ìœ íš¨ì„± ì ê²€
#     if (
#         second_cluster_num >= len(book_values_of_cluster) or 
#         third_cluster_num >= len(book_values_of_cluster)
#     ):
#         print(f"âŒ í´ëŸ¬ìŠ¤í„° ë²ˆí˜¸({second_cluster_num}, {third_cluster_num})ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ ë°˜í™˜.")
#         return np.zeros((1, book_embeddings.shape[1]))

#     second_cluster_indices = book_values_of_cluster[second_cluster_num]
#     third_cluster_indices = book_values_of_cluster[third_cluster_num]

#     print(f"first_cluster_num: {first_cluster_num}")
#     print(f"second_cluster_num: {second_cluster_num}")
#     print(f"third_cluster_num: {third_cluster_num}")
#     print(f"weight_first: {weight_first}, weight_second: {weight_second}, weight_third: {weight_third}")

#     # âœ… í´ëŸ¬ìŠ¤í„°ë³„ í‰ê·  ë²¡í„° ê³„ì‚°
#     if len(first_cluster_indices) == 0 or len(second_cluster_indices) == 0 or len(third_cluster_indices) == 0:
#         print("âŒ ì¼ë¶€ í´ëŸ¬ìŠ¤í„°ì— ì±…ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ ë°˜í™˜.")
#         return np.zeros((1, book_embeddings.shape[1]))

#     centroid_first = np.mean(book_embeddings[first_cluster_indices], axis=0)
#     centroid_second = np.mean(book_embeddings[second_cluster_indices], axis=0)
#     centroid_third = np.mean(book_embeddings[third_cluster_indices], axis=0)

#     # âœ… ê°€ì¤‘ í‰ê·  ê³„ì‚°
#     weighted_centroid = (
#         centroid_first * weight_first +
#         centroid_second * weight_second +
#         centroid_third * weight_third
#     ) / (weight_first + weight_second + weight_third)

#     # ğŸ›¡ï¸ NumPy ë°°ì—´ ë³€í™˜ ë° ì°¨ì› ì •ë¦¬
#     if isinstance(weighted_centroid, list):
#         weighted_centroid = np.array(weighted_centroid)

#     if len(weighted_centroid.shape) == 1:
#         weighted_centroid = weighted_centroid.reshape(1, -1)

#     print("âœ… 1ìœ„ í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì :", centroid_first)
#     print("âœ… 2ìœ„ í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì :", centroid_second)
#     print("âœ… 3ìœ„ í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì :", centroid_third)
#     print("âœ… ê°€ì¤‘ í‰ê·  ì¤‘ì‹¬ì :", weighted_centroid)
#     print(f"âœ… shape of weighted_centroid: {weighted_centroid.shape}")

#     return weighted_centroid


def neighborhood_based_clustering(weighted_centroid, book_embeddings):
    # âœ… 1. ì…ë ¥ ë²¡í„° ì°¨ì› í™•ì¸
    assert weighted_centroid.shape[1] == book_embeddings.shape[1], "ì°¨ì›ì´ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."

    # âœ… 2. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ë° ìƒìœ„ 300ê°œ ì„ íƒ
    cosine_similarities = cosine_similarity(book_embeddings, weighted_centroid).flatten()
    
    # âœ… ì„±ëŠ¥ í–¥ìƒ: np.argpartition() ì‚¬ìš©
    top_300_indices = np.argpartition(cosine_similarities, -300)[-300:]
    selected_vectors = book_embeddings[top_300_indices]

    # âœ… 3. ì„ íƒëœ ë²¡í„° ì •ê·œí™” (ì½”ì‚¬ì¸ ê±°ë¦¬ ê¸°ë°˜ KMeans)
    normalized_vectors = normalize(selected_vectors)

    # âœ… 4. KMeans í´ëŸ¬ìŠ¤í„°ë§ (ìµœì‹  ì˜µì…˜ ì ìš©)
    num_clusters = 5
    kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init='auto')
    clusters = kmeans.fit_predict(normalized_vectors)

    # ê° í´ëŸ¬ìŠ¤í„°ì˜ ì±… ì¸ë±ìŠ¤ ì €ì¥
    cluster_to_books = {i: [] for i in range(num_clusters)}
    for idx, cluster_id in enumerate(clusters):
        cluster_to_books[cluster_id].append(idx)

    # âœ… 5. í´ëŸ¬ìŠ¤í„° ê²°ê³¼ ì¶œë ¥
    unique, counts = np.unique(clusters, return_counts=True)
    print("K-Means í´ëŸ¬ìŠ¤í„° ê²°ê³¼:")
    for label, count in zip(unique, counts):
        print(f"í´ëŸ¬ìŠ¤í„° {label}: {count}ê°œ")

    return cluster_to_books, top_300_indices

def get_tournament_winner_cluster_until_round5(book_embeddings, cluster_to_books, alpha, beta_values, 
                                        presented_books, exploration_prob, noise_factor, book_choice, 
                                        question_number, books_chosen, suggested_books):

    if question_number == 0:
        book_a = int(np.random.choice([
            idx for idx in cluster_to_books[0] if idx not in presented_books]))
        
        book_b = int(np.random.choice([
            idx for idx in cluster_to_books[1] if idx not in presented_books]))

    elif question_number == 1:
        book_a = int(np.random.choice([
            idx for idx in cluster_to_books[2] if idx not in presented_books]))
        
        book_b = int(np.random.choice([
            idx for idx in cluster_to_books[3] if idx not in presented_books]))
        
    elif question_number == 2:
        book_a = int(np.random.choice([
            idx for idx in cluster_to_books[4] if idx not in presented_books]))
        
        book_b = int(np.random.choice([
            idx for idx in cluster_to_books[5] if idx not in presented_books]))

    elif question_number == 3:
        winner_of_1 = books_chosen[0]
        cluster_of_winner_1 = None

        for cluster_id, book_list in cluster_to_books.items():
            if winner_of_1 in book_list:
                cluster_of_winner_1 = cluster_id
                break
        print("winner_of_1: ", winner_of_1)
        print("cluster_of_winner_1", cluster_of_winner_1)


        winner_of_2 = books_chosen[1]
        cluster_of_winner_2 = None

        for cluster_id, book_list in cluster_to_books.items():
            if winner_of_2 in book_list:
                cluster_of_winner_2 = cluster_id
                break
        print("winner_of_2: ", winner_of_2)
        print("cluster_of_winner_2", cluster_of_winner_2)

        book_a = int(np.random.choice([
            idx for idx in cluster_to_books[cluster_of_winner_1] if idx not in presented_books]))
        book_b = int(np.random.choice([
            idx for idx in cluster_to_books[cluster_of_winner_2] if idx not in presented_books]))
        
    elif question_number == 4:
        winner_of_3 = books_chosen[2]
        cluster_of_winner_3 = None

        for cluster_id, book_list in cluster_to_books.items():
            if winner_of_3 in book_list:
                cluster_of_winner_3 = cluster_id
                break
        print("winner_of_3: ", winner_of_3)
        print("cluster_of_winner_3", cluster_of_winner_3)

        winner_of_4 = books_chosen[3]
        cluster_of_winner_4 = None

        for cluster_id, book_list in cluster_to_books.items():
            if winner_of_4 in book_list:
                cluster_of_winner_4 = cluster_id
                break
        print("winner_of_4: ", winner_of_4)
        print("cluster_of_winner_4", cluster_of_winner_4)

        book_a = int(np.random.choice([
            idx for idx in cluster_to_books[cluster_of_winner_3] if idx not in presented_books]))
        book_b = int(np.random.choice([
            idx for idx in cluster_to_books[cluster_of_winner_4] if idx not in presented_books]))
           

    else:
        raise ValueError("Question number out of range 5")
    
    winner_of_q = {f"winner_of_{question_number}"}

    print("suggested_books : ", suggested_books)
    print("presented_books : ", presented_books)
    print("books_chosen : ", books_chosen)
    presented_books.add(book_a)
    presented_books.add(book_b)
    suggested_books.append(book_a)
    suggested_books.append(book_b) 


    return book_a, book_b



def update_data(choice, book_a, book_b, alpha, beta_values):
    """
    ì‚¬ìš©ì ì„ íƒ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë² íƒ€ ë¶„í¬ ì—…ë°ì´íŠ¸.
    """
    print("book_a:", book_a)
    print("book_b:", book_b)
    if choice == "a":
        alpha[book_a] += 1
        beta_values[book_b] += 1
        # alpha[-1] += 1
        # alpha[0] += 1
        print(book_a)
        return book_a
    elif choice == "b":
        alpha[book_b] += 1
        beta_values[book_a] += 1
        return book_b
    else:
        beta_values[book_a] += 1
        beta_values[book_b] += 1
        return None
        


def get_message_by_id(ids, book_id, book_data):
    """
    idsì™€ book_dataë¥¼ ì´ìš©í•´ íŠ¹ì • book_idì˜ ë©”ì‹œì§€ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    idx = np.where(ids == book_id)[0][0]  # book_idì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
    return book_data[idx]["sentence"]


def weighted_sampling(similarities, num_samples=10, temperature=0.5):
    """
    ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í™•ë¥ ì  ìƒ˜í”Œë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    - similarities: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë°°ì—´
    - num_samples: ì¶”ì²œí•  ì±…ì˜ ê°œìˆ˜
    - temperature: ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜ ì¡°ì •ì„ ìœ„í•œ íŒŒë¼ë¯¸í„° (ë‚®ì„ìˆ˜ë¡ ìƒìœ„ ì„ íƒ ì§‘ì¤‘)
    """
    # ìœ ì‚¬ë„ë¥¼ ê°€ì¤‘ì¹˜ë¡œ ë³€í™˜
    probabilities = np.exp(similarities / temperature)
    probabilities /= probabilities.sum()  # í™•ë¥ ë¡œ ì •ê·œí™”

    # ê°€ì¤‘ì¹˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëœë¤ ìƒ˜í”Œë§
    sampled_indices = np.random.choice(len(similarities), size=num_samples, replace=False, p=probabilities)
    return sampled_indices
